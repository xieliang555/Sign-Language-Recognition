{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data import Field\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import PhoenixDataset\n",
    "from models.fine_grained_cnn import attention_net, list_loss, ranking_loss\n",
    "\n",
    "PROPOSAL_NUM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSZ = 2\n",
    "FrameSize = 224\n",
    "root = '/mnt/data/public/datasets'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomResizedCrop(FrameSize, scale=(0.8, 1)),\n",
    "     transforms.ToTensor()])\n",
    "\n",
    "TRG = Field(sequential=True, use_vocab=True,\n",
    "            init_token=None, eos_token=None,\n",
    "            lower=True, tokenize='spacy',\n",
    "            tokenizer_language='de')\n",
    "\n",
    "csv_path = os.path.join(root, 'phoenix2014-release/phoenix-2014-multisigner')\n",
    "csv_path = os.path.join(csv_path, 'annotations/manual/train.corpus.csv')\n",
    "csv_file = pd.read_csv(csv_path)\n",
    "train_sents = [csv_file.iloc[i, 0].lower().split('|')[3].split()\n",
    "               for i in range(len(csv_file))]\n",
    "TRG.build_vocab(train_sents, min_freq=1)\n",
    "VocabSize = len(TRG.vocab)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    videos = [item['video'] for item in batch]\n",
    "    videos = pad_sequence(videos, batch_first=True)\n",
    "    annotations = [item['annotation'].split() for item in batch]\n",
    "    annotations = TRG.process(annotations)\n",
    "    return {'videos': videos, 'annotations': annotations}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    PhoenixDataset(root, 'train', transform),\n",
    "    batch_size=BSZ, num_workers=BSZ, pin_memory=True,\n",
    "    shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    PhoenixDataset(root, 'dev', transform),\n",
    "    batch_size=1, num_workers=1, pin_memory=True,\n",
    "    shuffle=False, collate_fn=my_collate)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    PhoenixDataset(root, 'test', transform),\n",
    "    batch_size=1, num_workers=1, pin_memory=True,\n",
    "    shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fine_grained_cnn_net, cnn_rnn_ctc_net, train_loader,\n",
    "          optimizer, criterion, epoch, writer):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    fine_grained_cnn_net.eval()\n",
    "    cnn_rnn_ctc_net.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch['videos'].cuda()\n",
    "        targets = cnn_rnn_ctc_net(inputs)\n",
    "        targets = targets.max(-1)[1].permute(1, 0).contiguous().view(-1).data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        n, t, c, h, w = inputs.size()\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        raw_logits, concat_logits, part_logits, _, top_n_prob, _ = fine_grained_cnn_net(inputs)\n",
    "        # part_logits shape改变？\n",
    "        raw_loss = criterion(raw_logits, targets)\n",
    "        concat_loss = criterion(concat_logits, targets)\n",
    "        partcls_loss = criterion(part_logits, targets.unsqueeze(\n",
    "            1).repeat(1, PROPOSAL_NUM).view(-1))\n",
    "        part_loss = list_loss(part_logits, targets.unsqueeze(\n",
    "            1).repeat(1, PROPOSAL_NUM).view(-1)).view(-1, PROPOSAL_NUM)\n",
    "        rank_loss = ranking_loss(top_n_prob, part_loss)\n",
    "        total_loss =raw_loss + concat_loss + partcls_loss + rank_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        outs = concat_logits.max(-1)[1]\n",
    "        running_acc += targets.eq(outs).sum().item()/len(targets)\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        \n",
    "        N = len(train_loader) // 10\n",
    "        if batch_idx % N == N-1:\n",
    "            writer.add_scalar('train acc',\n",
    "                              running_acc/N,\n",
    "                              epoch*len(train_loader)+batch_idx)\n",
    "            writer.add_scalar('train loss',\n",
    "                              running_loss/N,\n",
    "                              epoch*len(train_loader)+batch_idx)\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(fine_grained_cnn_net, cnn_rnn_ctc_net, dev_loader, \n",
    "        criterion, epoch, writer):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    fine_grained_cnn_net.eval()\n",
    "    cnn_rnn_ctc_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dev_loader):\n",
    "            inputs = batch['videos'].cuda()\n",
    "            targets = cnn_rnn_ctc_net(inputs)\n",
    "            targets = targets.max(-1)[1].permute(1,0).contiguous().view(-1).data\n",
    "            \n",
    "            n,t,c,h,w = inputs.size()\n",
    "            inputs = inputs.view(-1,c,h,w)\n",
    "            raw_logits, concat_logits, part_logits, _, top_n_prob, _ = fine_grained_cnn_net(inputs)\n",
    "            raw_loss = criterion(raw_logits, targets)\n",
    "            concat_loss = criterion(concat_logits, targets)\n",
    "            partcls_loss = criterion(part_logits, targets.unsqueeze(\n",
    "                1).repeat(1, PROPOSAL_NUM).view(-1))\n",
    "            part_loss = list_loss(part_logits, targets.unsqueeze(\n",
    "                1).repeat(1, PROPOSAL_NUM).view(-1)).view(-1, PROPOSAL_NUM)\n",
    "            rank_loss = ranking_loss(top_n_prob, part_loss)\n",
    "            total_loss = raw_loss + concat_loss + partcls_loss + rank_loss\n",
    "            \n",
    "            outs = concat_logits.max(-1)[1].permute(1,0).contiguous().view(-1)\n",
    "            epoch_acc += targets.eq(outs).sum().item()/len(targets)\n",
    "            epoch_loss += total_loss.item()\n",
    "            \n",
    "    epoch_acc /= len(dev_loader)\n",
    "    epoch_loss /= len(dev_loader)\n",
    "    if writer:\n",
    "        writer.add_scalar('dev acc', epoch_acc, epoch)\n",
    "        writer.add_scalar('dev loss', epoch_loss, epoch)\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "save_dict1 = torch.load(\n",
    "    '/home/xieliang/Data/sign-language-recognition/save/CNN_RNN_CTC.pth')\n",
    "cnn_rnn_ctc_net = save_dict1['net'].cuda()\n",
    "for parmas in cnn_rnn_ctc_net.parameters():\n",
    "    parmas.requires_grad=False\n",
    "\n",
    "save_root = '/home/xieliang/Data/sign-language-recognition'\n",
    "save_model = os.path.join(save_root, 'save/fine_grained_cnn.pth')\n",
    "save_log = os.path.join(save_root, 'log/fine_grained_cnn')\n",
    "resume_training=False\n",
    "if resume_training:\n",
    "    save_dict2 = torch.load(save_model)\n",
    "    start_epoch = save_dict2['epoch']+1\n",
    "    best_dev_acc = save_dict2['best_dev_acc']\n",
    "    fine_grained_cnn_net = save_dict2['net'].cuda()\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_dev_acc = 0\n",
    "    fine_grained_cnn_net = attention_net(VocabSize, PROPOSAL_NUM).cuda()\n",
    "    \n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fine_grained_cnn_net.parameters(), lr=LR, weight_decay=WD)\n",
    "writer = SummaryWriter(save_log)\n",
    "\n",
    "print(f'start training in epoch {start_epoch} with best dev acc {best_dev_acc}')\n",
    "for epoch in range(start_epoch, 1000):\n",
    "    train(fine_grained_cnn_net, cnn_rnn_ctc_net, train_loader,\n",
    "          optimizer, criterion, epoch, writer)\n",
    "    dev_acc, dev_loss = val(fine_grained_cnn_net, cnn_rnn_ctc_net, \n",
    "        dev_loader, criterion, epoch, writer)\n",
    "    print(f'epoch: {epoch} | dev acc: {dev_acc} | dev loss: {dev_loss}')\n",
    "    \n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_dev_acc = dev_acc\n",
    "        torch.save({'epoch': epoch, 'best_dev_acc':best_dev_acc, \n",
    "                    'net':fine_grained_cnn_net}, save_model)\n",
    "        print(f'model saved with best dev acc {best_dev_acc} in epoch {epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bbx没有指向预期：\n",
    "    - 224->448, anchor size x2\n",
    "    - resnet18 -> restnet50?\n",
    "    - 第一个batch偶然原因全是bad case，多尝试其他的batch\n",
    "- ipynb经常导致断网\n",
    "    - 不要在ipynb中输出很大的数据流如图片后退出，这样会导致文件很大，加载很慢\n",
    "- rank loss太大：\n",
    "    - 增加rank loss权重\n",
    "    - 增加anchor size\n",
    "- 为什么框的大小基本一样\n",
    "- 尝试去除raw_logits进行预测\n",
    "- 尝试额外手动标记的区域训练cnn_rnn_ctc网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
