{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchtext.data import Field\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import utils\n",
    "import models\n",
    "from dataset import PhoenixDataset, ToTensorVideo, RandomResizedCropVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '/mnt/data/public/datasets'\n",
    "# print('video length: maximum / minimum / average / std')\n",
    "# print(utils.DatasetStatistic(root, 'train'))\n",
    "# print(utils.DatasetStatistic(root, 'dev'))\n",
    "# print(utils.DatasetStatistic(root, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG = Field(sequential=True, use_vocab=True,\n",
    "            init_token='<sos>', eos_token= '<eos>',\n",
    "            lower=True, tokenize='spacy',\n",
    "            tokenizer_language='de')\n",
    "\n",
    "root = '/mnt/data/public/datasets'\n",
    "csv_file = utils.get_csv(root)\n",
    "tgt_sents = [csv_file.iloc[i, 0].lower().split('|')[3].split()\n",
    "             for i in range(len(csv_file))]\n",
    "\n",
    "# hyper\n",
    "TRG.build_vocab(tgt_sents, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    process the batch:\n",
    "        pad the video to fixed frame length\n",
    "        convert sentence to index\n",
    "        video: [C, T, H, W]\n",
    "    '''\n",
    "    videos = [item['video'].permute(1,0,2,3) for item in batch]\n",
    "    videos= pad_sequence([v for v in videos], batch_first=True)\n",
    "    videos = videos.permute(0, 2, 1, 3 , 4)\n",
    "    \n",
    "    annotations = [item['annotation'].split() for item in batch]\n",
    "    annotations = TRG.process(annotations)\n",
    "\n",
    "    return {'videos': videos, 'annotations': annotations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSZ = 16\n",
    "root = '/mnt/data/public/datasets'\n",
    "transform = transforms.Compose([ToTensorVideo(),\n",
    "                                RandomResizedCropVideo(112)])\n",
    "\n",
    "train_loader = DataLoader(PhoenixDataset(root, 'train', transform=transform),\n",
    "                          batch_size=BSZ, shuffle=True, num_workers=10, collate_fn=collate_fn)\n",
    "\n",
    "dev_loader = DataLoader(PhoenixDataset(root, 'dev', transform=transform),\n",
    "                        batch_size=BSZ, shuffle=True, num_workers=10, collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(PhoenixDataset(root, 'test', transform=transform),\n",
    "                         batch_size=BSZ, shuffle=True, num_workers=10, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n",
      "34\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# videos: [N, C, T, H, W]\n",
    "# annotations: [L, N]\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# print(batch['videos'].shape)\n",
    "# print(batch['annotations'].shape)\n",
    "\n",
    "# print(utils.itos(batch['annotations'].squeeze(1), TRG))\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(dev_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 512\n",
    "DROPOUT = 0.1\n",
    "NHEAD = 8\n",
    "NLAYER = 6\n",
    "NHID = 1024\n",
    "ACTIVATION = 'relu'\n",
    "CLIP_SIZE = 10\n",
    "NEPOCH = 50\n",
    "LR=1e-4\n",
    "SEGMENT = 'OVERLAP'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = f'bsz:{BSZ}-lr:{LR}-epoch:{NEPOCH}-dmodel:{D_MODEL}-dropout:{DROPOUT}\\\n",
    "-nhead:{NHEAD}-nlayer:{NLAYER}-nhid:{NHID}-activation:{ACTIVATION}-clip_size:{CLIP_SIZE}-segment:{SEGMENT}'\n",
    "writer = SummaryWriter(os.path.join('./log', path))\n",
    "\n",
    "res3d = torchvision.models.video.r3d_18(pretrained=True)\n",
    "\n",
    "encoder = models.Res3D(res3d)\n",
    "\n",
    "decoder = models.Transformer(\n",
    "    device, len(TRG.vocab), D_MODEL, DROPOUT,\n",
    "    NHEAD, NLAYER, NHID, ACTIVATION)\n",
    "\n",
    "model = models.Seq2Seq(CLIP_SIZE, encoder, decoder, device).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi['<pad>'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, torch.zeros(1,3,100,112,112).to(device), \n",
    "#         torch.zeros(10,1, dtype=torch.long).to(device),\n",
    "#         torch.zeros(1,100,dtype=torch.bool).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 28s, sys: 16min 51s, total: 36min 19s\n",
      "Wall time: 1h 23min 20s\n",
      "CPU times: user 1min 50s, sys: 1min 29s, total: 3min 20s\n",
      "Wall time: 3min 43s\n",
      "0 4.601124006159165 0.04133609704235021 0.8911424780576237\n",
      "CPU times: user 20min 15s, sys: 16min 38s, total: 36min 54s\n",
      "Wall time: 48min 3s\n",
      "CPU times: user 1min 51s, sys: 1min 35s, total: 3min 27s\n",
      "Wall time: 2min 39s\n",
      "1 4.245331091039321 0.0513331392670379 0.8023410824316906\n",
      "CPU times: user 20min 33s, sys: 17min 4s, total: 37min 37s\n",
      "Wall time: 45min 53s\n",
      "CPU times: user 1min 50s, sys: 1min 32s, total: 3min 22s\n",
      "Wall time: 3min 46s\n",
      "2 4.0913653513964485 0.047211908132714385 0.7929021234537266\n",
      "CPU times: user 20min 35s, sys: 17min 5s, total: 37min 40s\n",
      "Wall time: 48min 39s\n",
      "CPU times: user 1min 51s, sys: 1min 33s, total: 3min 25s\n",
      "Wall time: 3min 40s\n",
      "3 4.081003504640916 0.0458398761556429 0.7848909117338055\n",
      "CPU times: user 20min 28s, sys: 17min 7s, total: 37min 36s\n",
      "Wall time: 42min 24s\n",
      "CPU times: user 1min 51s, sys: 1min 34s, total: 3min 26s\n",
      "Wall time: 3min 26s\n",
      "4 4.05080655743094 0.044979042647516024 0.7845889590377757\n",
      "CPU times: user 20min 29s, sys: 17min 7s, total: 37min 37s\n",
      "Wall time: 44min 54s\n",
      "CPU times: user 1min 49s, sys: 1min 36s, total: 3min 26s\n",
      "Wall time: 4min 9s\n",
      "5 4.042183981222265 0.05762835964560509 0.7744599436348063\n",
      "CPU times: user 20min 28s, sys: 16min 59s, total: 37min 27s\n",
      "Wall time: 43min 26s\n",
      "CPU times: user 1min 51s, sys: 1min 30s, total: 3min 21s\n",
      "Wall time: 4min 1s\n",
      "6 3.9789840824463787 0.06897791486014337 0.767138063638241\n",
      "CPU times: user 20min 24s, sys: 17min 28s, total: 37min 52s\n",
      "Wall time: 49min 1s\n",
      "CPU times: user 1min 50s, sys: 1min 35s, total: 3min 25s\n",
      "Wall time: 3min 55s\n",
      "7 3.9587291829726277 0.06811358713928391 0.7761385751360721\n"
     ]
    }
   ],
   "source": [
    "best_val_bleu = 0.0\n",
    "best_val_model = copy.deepcopy(model.state_dict())\n",
    "for n_epoch in range(NEPOCH):\n",
    "    %time models.train(model, train_loader, device, criterion, optimizer, TRG, writer, n_epoch)\n",
    "    %time val_loss, val_bleu, val_wer = models.evaluate(model, dev_loader, device, criterion, TRG)\n",
    "    print(n_epoch, val_loss, val_bleu, val_wer)\n",
    "    \n",
    "    if val_bleu > best_val_bleu:\n",
    "        best_val_bleu = val_bleu\n",
    "        best_val_model = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "model.load_state_dict(best_val_model)\n",
    "test_loss, test_bleu, test_wer = models.evaluate(model, test_loader, device, criterion, TRG, test_mode = True)\n",
    "print(test_loss, test_bleu, test_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('./save'):\n",
    "#     os.mkdir(\"save\")\n",
    "# dir_name = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
    "# torch.save(model.state_dict(), './save/'+dir_name+'.pth')\n",
    "\n",
    "\n",
    "# # change input shape from [N, C, T, H, W] to [N, T, C, H, W]\n",
    "# videos = batch['videos'].permute(0, 2, 1, 3, 4)\n",
    "# texts = batch['annotations'].permute(1, 0)\n",
    "# texts = [' '.join([TRG.vocab.itos[i] for i in sent]) for sent in texts]\n",
    "# writer.add_video('input', videos, global_step=0, fps=32)\n",
    "# writer.add_text('annotations', str(texts), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T02:10:31.846167Z",
     "start_time": "2020-03-03T02:10:31.844043Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model.load_state_dict(torch.load('./save/2020-03-01 18:17:57.pth'))\n",
    "# test_loss, test_bleu, test_wer = models.evaluate(model, test_loader, device, criterion, TRG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
