{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchtext.data import Field\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import utils\n",
    "from dataset import PhoenixDataset, ToTensorVideo, RandomResizedCropVideo\n",
    "from models import cnn, transformer, seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video length: maximum / minimum / average / std\n",
      "[299, 16, 140.8684767277856, 42.786819204979956]\n",
      "[251, 32, 139.23333333333332, 44.30025081443922]\n",
      "[251, 39, 142.24483306836248, 43.06641256741527]\n"
     ]
    }
   ],
   "source": [
    "# root = '/mnt/data/public/datasets'\n",
    "# print('video length: maximum / minimum / average / std')\n",
    "# print(utils.DatasetStatistic(root, 'train'))\n",
    "# print(utils.DatasetStatistic(root, 'dev'))\n",
    "# print(utils.DatasetStatistic(root, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235\n"
     ]
    }
   ],
   "source": [
    "TRG = Field(sequential=True, use_vocab=True,\n",
    "            init_token='<sos>', eos_token= '<eos>',\n",
    "            lower=True, tokenize='spacy',\n",
    "            tokenizer_language='de')\n",
    "\n",
    "root = '/mnt/data/public/datasets'\n",
    "csv_file = utils.get_csv(root)\n",
    "tgt_sents = [csv_file.iloc[i, 0].lower().split('|')[3].split()\n",
    "             for i in range(len(csv_file))]\n",
    "\n",
    "\n",
    "TRG.build_vocab(tgt_sents, min_freq=1)\n",
    "VOCAB_SIZE = len(TRG.vocab)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    process the batch:\n",
    "        pad the video to fixed frame length\n",
    "        convert sentence to index\n",
    "        video: [C, T, H, W]\n",
    "    '''\n",
    "    videos = [item['video'].permute(1,0,2,3) for item in batch]\n",
    "    videos= pad_sequence([v for v in videos], batch_first=True)\n",
    "    videos = videos.permute(0, 2, 1, 3 , 4)\n",
    "    \n",
    "    annotations = [item['annotation'].split() for item in batch]\n",
    "    annotations = TRG.process(annotations)\n",
    "\n",
    "    return {'videos': videos, 'annotations': annotations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSZ = 4\n",
    "SIZE = 224\n",
    "root = '/mnt/data/public/datasets'\n",
    "transform = transforms.Compose([ToTensorVideo(),\n",
    "                                RandomResizedCropVideo(SIZE)])\n",
    "\n",
    "# ? shuffle false to use smaller dataset\n",
    "train_loader = DataLoader(PhoenixDataset(root, 'train', transform=transform),\n",
    "                          batch_size=BSZ, shuffle=False, num_workers=10, collate_fn=collate_fn)\n",
    "\n",
    "dev_loader = DataLoader(PhoenixDataset(root, 'dev', transform=transform),\n",
    "                        batch_size=BSZ, shuffle=False, num_workers=10, collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(PhoenixDataset(root, 'test', transform=transform),\n",
    "                         batch_size=BSZ, shuffle=False, num_workers=10, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418\n",
      "135\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "# videos: [N, C, T, H, W]\n",
    "# annotations: [L, N]\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# print(batch['videos'].shape)\n",
    "# print(batch['annotations'].shape)\n",
    "\n",
    "# print(utils.itos(batch['annotations'].squeeze(1), TRG))\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(dev_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 512\n",
    "DROPOUT = 0.1\n",
    "NHEAD = 8\n",
    "NLAYER = 6\n",
    "NHID = 1024\n",
    "ACTIVATION = 'relu'\n",
    "CLIP_SIZE = 10\n",
    "NEPOCH = 34\n",
    "LR=1e-4\n",
    "SEGMENT = 'OVERLAP'\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = f'bsz:{BSZ}-lr:{LR}-epoch:{NEPOCH}-size:{SIZE}-dmodel:{D_MODEL}-dropout:{DROPOUT}\\\n",
    "-nhead:{NHEAD}-nlayer:{NLAYER}-nhid:{NHID}-activation:{ACTIVATION}-clip_size:{CLIP_SIZE}-segment:{SEGMENT}'\n",
    "writer = SummaryWriter(os.path.join('./log', path))\n",
    "\n",
    "# res3d = torchvision.models.video.r3d_18(pretrained=True)\n",
    "res3d = torch.load('./save/res3d18.pth', map_location = device)\n",
    "CNN = cnn.Res3D(res3d)\n",
    "\n",
    "Transformer = transformer.Transformer(\n",
    "    len(TRG.vocab), D_MODEL, DROPOUT,\n",
    "    NHEAD, NLAYER, NHID, ACTIVATION)\n",
    "\n",
    "# ?\n",
    "# model = seq2seq.Res3d_transformer(CLIP_SIZE, CNN, Transformer).to(device)\n",
    "model = torch.load('./save/res3d18_transformer.pth', map_location=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi['<pad>'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, batch in enumerate(test_loader):\n",
    "#     print(batch['videos'].shape)\n",
    "#     print(batch['annotations'].shape)\n",
    "#     video = batch['videos'].permute(0,2,1,3,4)\n",
    "#     annotation = batch['annotations'].permute(1,0)[0]\n",
    "#     annotation = ' '.join([TRG.vocab.itos[i] for i in annotation])\n",
    "    \n",
    "#     writer.add_video('test', video, global_step=0, fps=4)\n",
    "    \n",
    "#     if batch_idx == 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, torch.zeros(1,3,100,112,112).to(device), \n",
    "#         torch.zeros(10,1, dtype=torch.long).to(device),\n",
    "#         torch.zeros(1,100,dtype=torch.bool).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, criterion, optimizer, TRG, writer, n_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_bleu = 0.0\n",
    "    running_wer = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch['videos'].to(device)\n",
    "        targets = batch['annotations'].to(device)\n",
    "        # ? 改进 通过collate_fn返回\n",
    "        src_padding_mask = transformer.get_padding_mask(inputs).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, targets[:-1, :], src_padding_mask, device)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                         targets[1:, :].view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_bleu += utils.bleu_count(outputs, targets[1:, :], TRG)\n",
    "        running_wer += utils.wer_count(outputs, targets[1:, :], TRG)\n",
    "        \n",
    "        # ?\n",
    "        if batch_idx == 90:\n",
    "            break\n",
    "        \n",
    "            \n",
    "        # 34 -> 9\n",
    "        if batch_idx % 9 == 8:\n",
    "            writer.add_scalar('train loss',\n",
    "                              running_loss / 9,\n",
    "                              # ？\n",
    "                              n_epoch * len(train_loader) + batch_idx)\n",
    "            writer.add_scalar('train bleu',\n",
    "                              running_bleu / 9,\n",
    "                              n_epoch * len(train_loader) + batch_idx)\n",
    "            writer.add_scalar('train wer',\n",
    "                              running_wer / 9,\n",
    "                              n_epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_bleu = 0.0\n",
    "            running_wer = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_loader, device, criterion, TRG, writer, n_epoch):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_bleu = 0.0\n",
    "    epoch_wer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dev_loader):\n",
    "            inputs = batch['videos'].to(device)\n",
    "            targets = batch['annotations'].to(device)\n",
    "            # validation and test share the same src_padding_mask\n",
    "            src_padding_mask = transformer.get_padding_mask(inputs).to(device)\n",
    "            outputs = model(inputs, targets[:-1,:], src_padding_mask, device)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                             targets[1:, :].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bleu += utils.bleu_count(outputs, targets[1:, :], TRG)\n",
    "            epoch_wer += utils.wer_count(outputs, targets[1:, :], TRG)\n",
    "            \n",
    "            # ?\n",
    "            if batch_idx == 9:\n",
    "                break\n",
    "    \n",
    "    # len(dev_loader) -> 10\n",
    "    epoch_loss /= 10\n",
    "    epoch_bleu /= 10\n",
    "    epoch_wer /= 10\n",
    "    \n",
    "    # if ?\n",
    "    if writer:\n",
    "        writer.add_scalar('val loss', epoch_loss, n_epoch)\n",
    "        writer.add_scalar('val bleu', epoch_bleu, n_epoch)\n",
    "        writer.add_scalar('val wer', epoch_wer, n_epoch)\n",
    "    return epoch_loss, epoch_bleu, epoch_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device, criterion, TRG):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_bleu = 0.0\n",
    "    epoch_wer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            inputs = batch['videos'].to(device)\n",
    "            targets = batch['annotations'].to(device)\n",
    "            src_padding_mask = transformer.get_padding_mask(inputs).to(device)\n",
    "            dec_inputs = transformer.greedy_decoder(model, inputs, targets[:-1, :], src_padding_mask, device).to(device)\n",
    "            outputs = model(inputs, dec_inputs, src_padding_mask, device)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets[1:,:].view(-1))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bleu += utils.bleu_count(outputs, targets[1:,:], TRG)\n",
    "            epoch_wer += utils.wer_count(outputs, targets[1:,:], TRG)\n",
    "            \n",
    "            if batch_idx == 9:\n",
    "                break\n",
    "          \n",
    "    # ? len(test_loader) -> 10\n",
    "    epoch_loss /= 10\n",
    "    epoch_bleu /= 10\n",
    "    epoch_wer /= 10\n",
    "                \n",
    "    return epoch_loss, epoch_bleu, epoch_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 24s, sys: 3min 52s, total: 8min 17s\n",
      "Wall time: 5min 12s\n",
      "CPU times: user 27.2 s, sys: 26 s, total: 53.1 s\n",
      "Wall time: 42.3 s\n",
      "epoch:0 | val loss:4.6117489576339725 | val bleu:0.009405815601348877 | val wer:0.8757083799419846\n",
      "best model params saved in epoch 0 with best val wer: 0.8757083799419846\n",
      "CPU times: user 5min 4s, sys: 4min 10s, total: 9min 14s\n",
      "Wall time: 5min 42s\n",
      "CPU times: user 36.1 s, sys: 28.9 s, total: 1min 4s\n",
      "Wall time: 45.5 s\n",
      "epoch:1 | val loss:4.616577982902527 | val bleu:0.010252672433853149 | val wer:0.8670510711452775\n",
      "best model params saved in epoch 1 with best val wer: 0.8670510711452775\n",
      "CPU times: user 5min 31s, sys: 4min 24s, total: 9min 56s\n",
      "Wall time: 6min 20s\n",
      "CPU times: user 37.1 s, sys: 31.3 s, total: 1min 8s\n",
      "Wall time: 46.8 s\n",
      "epoch:2 | val loss:4.672018146514892 | val bleu:0.020611317455768587 | val wer:0.8794950222961253\n",
      "CPU times: user 5min 32s, sys: 4min 11s, total: 9min 44s\n",
      "Wall time: 6min 20s\n",
      "CPU times: user 30.2 s, sys: 28.7 s, total: 58.8 s\n",
      "Wall time: 37.8 s\n",
      "epoch:3 | val loss:4.676525092124939 | val bleu:0.011304187774658202 | val wer:0.856067962128952\n",
      "best model params saved in epoch 3 with best val wer: 0.856067962128952\n",
      "CPU times: user 5min 17s, sys: 3min 58s, total: 9min 16s\n",
      "Wall time: 5min 59s\n",
      "CPU times: user 30.3 s, sys: 28.9 s, total: 59.2 s\n",
      "Wall time: 39.3 s\n",
      "epoch:4 | val loss:4.691996431350708 | val bleu:0.020075501501560213 | val wer:0.8660941210535323\n",
      "CPU times: user 4min 43s, sys: 4min 3s, total: 8min 47s\n",
      "Wall time: 5min 16s\n",
      "CPU times: user 32.1 s, sys: 25.3 s, total: 57.4 s\n",
      "Wall time: 38.9 s\n",
      "epoch:5 | val loss:4.756452679634094 | val bleu:0.014574402570724487 | val wer:0.8735570296701459\n",
      "CPU times: user 4min 45s, sys: 4min 5s, total: 8min 50s\n",
      "Wall time: 5min 17s\n",
      "CPU times: user 31 s, sys: 27.8 s, total: 58.8 s\n",
      "Wall time: 38.9 s\n",
      "epoch:6 | val loss:4.777545475959778 | val bleu:0.02031034082174301 | val wer:0.8754019140682218\n",
      "CPU times: user 4min 43s, sys: 3min 57s, total: 8min 40s\n",
      "Wall time: 5min 15s\n",
      "CPU times: user 30.9 s, sys: 25.8 s, total: 56.6 s\n",
      "Wall time: 38.4 s\n",
      "epoch:7 | val loss:4.779650712013245 | val bleu:0.011304187774658202 | val wer:0.870390992764197\n",
      "CPU times: user 4min 43s, sys: 4min 7s, total: 8min 50s\n",
      "Wall time: 5min 16s\n",
      "CPU times: user 31 s, sys: 28.5 s, total: 59.4 s\n",
      "Wall time: 38.2 s\n",
      "epoch:8 | val loss:4.782009720802307 | val bleu:0.015730665624141695 | val wer:0.8767267085858073\n",
      "CPU times: user 4min 42s, sys: 4min 8s, total: 8min 50s\n",
      "Wall time: 5min 15s\n",
      "CPU times: user 30.5 s, sys: 27 s, total: 57.5 s\n",
      "Wall time: 37.9 s\n",
      "epoch:9 | val loss:4.7912212133407595 | val bleu:0.015299279987812043 | val wer:0.8648678322612126\n",
      "CPU times: user 5min 46s, sys: 4min 19s, total: 10min 5s\n",
      "Wall time: 6min 38s\n",
      "CPU times: user 40.7 s, sys: 31.4 s, total: 1min 12s\n",
      "Wall time: 51.9 s\n",
      "epoch:10 | val loss:4.820579028129577 | val bleu:0.023224776983261107 | val wer:0.8677963651695695\n",
      "CPU times: user 5min 57s, sys: 4min 25s, total: 10min 23s\n",
      "Wall time: 6min 56s\n",
      "CPU times: user 40.1 s, sys: 31.3 s, total: 1min 11s\n",
      "Wall time: 50.8 s\n",
      "epoch:11 | val loss:4.832617926597595 | val bleu:0.022965461388230322 | val wer:0.8807799712927752\n",
      "CPU times: user 5min 17s, sys: 4min 3s, total: 9min 20s\n",
      "Wall time: 5min 58s\n",
      "CPU times: user 41.9 s, sys: 32.3 s, total: 1min 14s\n",
      "Wall time: 53.4 s\n",
      "epoch:12 | val loss:4.856228518486023 | val bleu:0.016676753759384155 | val wer:0.8602095926359883\n",
      "CPU times: user 8min 1s, sys: 5min 4s, total: 13min 5s\n",
      "Wall time: 9min 40s\n",
      "CPU times: user 45.3 s, sys: 34.3 s, total: 1min 19s\n",
      "Wall time: 1min 3s\n",
      "epoch:13 | val loss:4.928524684906006 | val bleu:0.017376029491424562 | val wer:0.8771902926499051\n",
      "CPU times: user 8min 39s, sys: 5min 20s, total: 14min\n",
      "Wall time: 10min 38s\n",
      "CPU times: user 1min 3s, sys: 38.5 s, total: 1min 41s\n",
      "Wall time: 1min 23s\n",
      "epoch:14 | val loss:4.947785377502441 | val bleu:0.017175117135047914 | val wer:0.8622417972544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Code/sign-language-recognition/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, device, criterion, optimizer, TRG, writer, n_epoch)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sign/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/sign-language-recognition/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_padding_mask, device)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sign/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/sign-language-recognition/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, tgt, src_padding_mask, device)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_wer = 0.9\n",
    "for n_epoch in range(NEPOCH):\n",
    "    %time train(model, train_loader, device, criterion, optimizer, TRG, writer, n_epoch)\n",
    "    %time val_loss, val_bleu, val_wer = evaluate(model, dev_loader, device, criterion, TRG, writer, n_epoch)\n",
    "    print(f'epoch:{n_epoch} | val loss:{val_loss} | val bleu:{val_bleu} | val wer:{val_wer}')\n",
    "    \n",
    "    if val_wer < best_val_wer:\n",
    "        best_val_wer = val_wer\n",
    "        torch.save(model, './save/res3d18_transformer.pth')\n",
    "        print(f'best model params saved in epoch {n_epoch} with best val wer: {best_val_wer}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T02:10:31.846167Z",
     "start_time": "2020-03-03T02:10:31.844043Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi['<pad>'])\n",
    "model_ft = torch.load('./save/res3d18_transformer.pth', map_location=device)\n",
    "\n",
    "test_loss, test_bleu, test_wer = test(model_ft, test_loader, device, criterion, TRG)\n",
    "print(f'test loss {test_loss} | test bleu {test_bleu} | test wer {test_wer}')\n",
    "\n",
    "# val_loss, val_bleu, val_wer = evaluate(model_ft, dev_loader, device, criterion, TRG, writer=None, n_epoch=0)\n",
    "# print(val_loss, val_bleu, val_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
