{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import utils\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def get_padding_mask(self, data):\n",
    "    if len(data.size()) == 2:\n",
    "        # tgt: [T, N]\n",
    "        # the index of tgt padding is 1\n",
    "        mask = data.eq(1).transpose(0, 1)\n",
    "    else if len(data.size()) == 5:\n",
    "        # src: [N, C, T, H, W]\n",
    "        # the index of src padding is 0\n",
    "        data = data.permute(0, 2, 1, 3, 4).sum(dim=(-3, -2, -1))\n",
    "        mask = data.eq(0).transpose(0, 1)\n",
    "\n",
    "    mask = mask.masked_fill(mask == True, float(\n",
    "        '-inf')).masked_fill(mask == False, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_subsequent_mask(self, tgt):\n",
    "    # torch.triu 上三角\n",
    "    seq_len = tgt.size(0)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1.0, float(\n",
    "        '-inf')).masked_fill(mask == 0.0, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Res3D(nn.Module):\n",
    "    def __init__(self, res3d):\n",
    "        '''\n",
    "        Args:\n",
    "            res3d: the pretrained model\n",
    "        '''\n",
    "        super(Res3D, self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            res3d.stem,\n",
    "            res3d.layer1,\n",
    "            res3d.layer2,\n",
    "            res3d.layer3,\n",
    "            res3d.layer4,\n",
    "            res3d.avgpool)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        d_model: the embdedding feature dimension\n",
    "    '''\n",
    "\n",
    "    def __init__(self, device, tgt_vocab_size, d_model=512, dropout=0.1,\n",
    "                 nhead=8, nlayer=6, nhid=2048, activation='relu'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=nlayer,\n",
    "            num_decoder_layers=nlayer,\n",
    "            dim_feedforward=nhid,\n",
    "            activation=activation)\n",
    "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt, src_padding_mask):\n",
    "        src_padding_mask = src_padding_mask.to(self.device)\n",
    "        memory_padding_mask = src_padding_mask.to(self.device)\n",
    "        tgt_padding_mask = get_padding_mask(tgt).to(self.device)\n",
    "        tgt_subsequent_mask = get_subsequent_mask(tgt).to(self.device)\n",
    "\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = self.dropout(self.pe(x))\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.dropout(self.pe(tgt))\n",
    "        out = self.transformer(\n",
    "            x, tgt, tgt_mask=tgt_subsequent_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=memory_padding_mask)\n",
    "\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 只需确定一维\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(- torch.arange(0, d_model,\n",
    "                                            2).float() * math.log(10000) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        nclip: the number of clips in a input video\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nclip, cnn, transformer, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.nclip = nclip\n",
    "        self.device = device\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        for params in self.cnn.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def forward(self, x, tgt, src_padding_mask):\n",
    "        # cnn_out: [T, N, E]\n",
    "        cnn_out = torch.zeros(self.nclip, x.size(\n",
    "            0), self.transformer.d_model).to(self.device)\n",
    "        x = x.chunk(self.nclip, 2)\n",
    "        for idx, clip in enumerate(x):\n",
    "            cnn_out[idx, :, :] = self.cnn(clip)\n",
    "\n",
    "        y = self.transformer(cnn_out, tgt, src_padding_mask)\n",
    "        return y\n",
    "\n",
    "\n",
    "# beam >1 ?\n",
    "def greedy_decoder(model, enc_inputs, targets, src_padding_mask, device):\n",
    "    \"\"\"\n",
    "        targets: ['sos','w1','w2'...'wn']\n",
    "        Beam search: K=1\n",
    "    \"\"\"\n",
    "    src_padding_mask = src_padding_mask.to(device)\n",
    "    memory_padding_mask = src_padding_mask.to(device)\n",
    "    tgt_padding_mask = get_padding_mask(targets).to(device)\n",
    "    tgt_subsequent_mask = get_subsequent_mask(targets).to(device)\n",
    "\n",
    "    enc_inputs = enc_inputs * math.sqrt(model.transformer.d_model)\n",
    "    enc_inputs = model.transformer.dropout(model.transformer.pe(enc_inputs))\n",
    "    enc_outputs = model.transformer.Transformer.encoder(\n",
    "        enc_inputs, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "    dec_inputs = targets.clone()\n",
    "    for i in range(1, len(dec_inputs)):\n",
    "        dec_inputs = model.transformer.embedding(\n",
    "            dec_inputs) * math.sqrt(model.transformer.d_model)\n",
    "        dec_inputs = model.transformer.dropout(\n",
    "            model.transformer.pe(dec_inputs))\n",
    "\n",
    "        dec_outputs = model.transformer.Transformer.decoder(\n",
    "            dec_inputs, enc_outputs, \n",
    "            tgt_mask=tgt_subsequent_mask, \n",
    "            tgt_key_padding_mask=tgt_padding_mask, \n",
    "            memory_key_padding_mask=memory_padding_mask)\n",
    "        \n",
    "        out = model.transformer.out(dec_outputs)\n",
    "        idx = out.max(dim=-1, keepdim=False)[1]\n",
    "        dec_inputs[i] = idx.data[i-1]\n",
    "    return dec_inputs\n",
    "\n",
    "\n",
    "def train(model, train_loader, device, criterion, optimizer, TRG, writer, n_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_bleu = 0.0\n",
    "    running_wer = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch['videos'].to(device)\n",
    "        targets = batch['annotations'].to(device)\n",
    "        # ?\n",
    "        src_padding_mask = get_padding_mask(inputs).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, targets[:-1, :], src_padding_mask)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                         targets[1:, :].view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_bleu += utils.bleu_count(outputs, targets[1:, :], TRG)\n",
    "        running_wer += utils.wer_count(outputs, targets[1:, :], TRG)\n",
    "\n",
    "        if batch_idx % 50 == 49:\n",
    "            writer.add_scalar('train loss',\n",
    "                              running_loss / 50,\n",
    "                              n_epoch * len(train_loader) + batch_idx)\n",
    "            writer.add_scalar('train bleu',\n",
    "                              running_bleu / 50,\n",
    "                              n_epoch * len(train_loader) + batch_idx)\n",
    "            writer.add_scalar('train wer',\n",
    "                              running_wer / 50,\n",
    "                              n_epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_bleu = 0.0\n",
    "            running_wer = 0.0\n",
    "\n",
    "\n",
    "def evaluate(model, dev_loader, device, criterion, TRG):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_bleu = 0.0\n",
    "    epoch_wer = 0.0\n",
    "    for batch_idx, batch in enumerate(dev_loader):\n",
    "        inputs = batch['videos'].to(device)\n",
    "        targets = batch['annotations'].to(device)\n",
    "        # ?\n",
    "        src_padding_mask = get_padding_mask(inputs).to(device)\n",
    "        # ?\n",
    "        dec_inputs = greedy_decoder(\n",
    "            model, inputs, targets[:-1, :],\n",
    "            src_padding_mask, device).to(device)\n",
    "        outputs = model(inputs, dec_inputs, src_padding_mask)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                         targets[1:, :].view(-1))\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_bleu += utils.bleu_count(outputs, targets[1:, :], TRG)\n",
    "        epoch_wer += utils.wer_count(outputs, targets[1:, :], TRG)\n",
    "\n",
    "    return epoch_loss / len(dev_loader), epoch_bleu / len(dev_loader), epoch_wer / (len(dev_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
