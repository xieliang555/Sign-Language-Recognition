{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data import Field\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from itertools import groupby\n",
    "from jiwer import wer\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from Core.cnn_rnn import CNN_RNN\n",
    "from Core.dataset import PhoenixDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG = Field(sequential=True, use_vocab=True,\n",
    "            init_token=None, eos_token= None,\n",
    "            lower=True, tokenize='spacy',\n",
    "            tokenizer_language='de')\n",
    "\n",
    "\n",
    "root = '/mnt/data/public/datasets'\n",
    "csv_dir = os.path.join(root, 'phoenix2014-release/phoenix-2014-multisigner')\n",
    "csv_dir = os.path.join(csv_dir, 'annotations/manual/train.corpus.csv')\n",
    "csv_file = pd.read_csv(csv_dir)\n",
    "tgt_sents = [csv_file.iloc[i, 0].lower().split('|')[3].split()\n",
    "             for i in range(len(csv_file))]\n",
    "TRG.build_vocab(tgt_sents, min_freq=1)\n",
    "VocabSize = len(TRG.vocab)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    videos = [item['video'] for item in batch]\n",
    "    video_lens = torch.tensor([len(v) for v in videos])\n",
    "    videos = pad_sequence(videos, batch_first=True)\n",
    "    annotations = [item['annotation'].split() for item in batch]\n",
    "    annotation_lens = torch.tensor([len(anno) for anno in annotations])\n",
    "    annotations = TRG.process(annotations)\n",
    "    return {'videos': videos,\n",
    "            'annotations': annotations,\n",
    "            'video_lens': video_lens,\n",
    "            'annotation_lens': annotation_lens}\n",
    "\n",
    "FrameSize = 224\n",
    "BSZ = 2\n",
    "interval = 4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(FrameSize),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    PhoenixDataset(root, mode='train', interval=interval, transform=transform),\n",
    "    batch_size=BSZ, shuffle=True, num_workers=BSZ,\n",
    "    collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    PhoenixDataset(root, mode='dev', interval=interval, transform=transform),\n",
    "    batch_size=BSZ, shuffle=False, num_workers=BSZ,\n",
    "    collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    PhoenixDataset(root, mode='test', interval=interval, transform=transform),\n",
    "    batch_size=BSZ, shuffle=False, num_workers=BSZ,\n",
    "    collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(net, test_loader, criterion):\n",
    "    net.eval()\n",
    "    epoch_wer = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            inputs = batch['videos'].cuda()\n",
    "            targets = batch['annotations'].permute(1,0).contiguous().cuda()\n",
    "            input_lens = batch['video_lens'].cuda()\n",
    "            target_lens = batch['annotation_lens'].cuda()\n",
    "            \n",
    "            outs = net(inputs)\n",
    "            loss = criterion(outs, targets, input_lens, target_lens)\n",
    "            \n",
    "            outs = outs.max(-1)[1].permute(1,0).contiguous().view(-1)\n",
    "            outs = ' '.join([TRG.vocab.itos[k] for k, _ in groupby(outs) if k != VocabSize])\n",
    "            targets = targets.view(-1)\n",
    "            targets = ' '.join([TRG.vocab.itos[k] for k in targets])\n",
    "            epoch_wer += wer(targets, outs, standardize=True)\n",
    "            epoch_loss += loss.item()\n",
    "          \n",
    "    return epoch_loss/len(test_loader), epoch_wer/len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "save_root = '/home/xieliang/Data/sign-language-recognition'\n",
    "save_model = os.path.join(save_root, 'save/CNN_RNN_CTC3.pth')\n",
    "\n",
    "save_dict = torch.load(save_model)\n",
    "best_dev_wer = save_dict['best_dev_wer']\n",
    "net = save_dict['net'].cuda()\n",
    "criterion = nn.CTCLoss(blank=VocabSize)\n",
    "\n",
    "test_loss, test_wer = val(net, test_loader, criterion)\n",
    "print(test_loss, test_wer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
